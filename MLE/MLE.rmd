---
title: "Numerical Optimization"
author: "Andrew Fogarty"
date: "5/23/2020"
output: 
     rmarkdown::html_vignette:
          toc: TRUE
          number_sections: TRUE
editor_options: 
  chunk_output_type: console
---


# Introduction

Maximum likelihood fixes our observed data and asks: What parameter values most likely generated the data we have? In a likelihood framework, our data is viewed as a joint probability as a function of parameter values for a specified mass or density function. In this case, the joint probability is being maximized with respect to the parameters. A maximum likelihood estimate is one that provides the density or mass function with the highest likelihood of generating the observed data.

Numerical optimizers provide the means to estimate our parameters by finding the values that maximize the likelihood of generating our data. This guide helps us understand how optimization algorithms find the best estimates for our coefficients in a step-by-step process.


# Numerical Optimization

In the chunk below, some binary synthetic data is created for use in logistic regression.

```{r}
y <- sample(c(0, 1), size =500, replace=TRUE)
x1 <- rnorm(500)
x2 <- rnorm(500)
intercept <- rep(1, 500)
X <- data.frame(intercept, x1, x2)
X <- data.matrix(X)
```

When calculating the MLE, most programs iterate some numerical optimization procedure. These algorithms continuously recalculate the parameters until the change in the current value falls below a specified tolerance -- when the algorithm is then said to have converged. The workhorse for finding extrema of log-likelihood functions are hill-climbing algorithms that use the information in the derivative of the log-likelihood function to climb to a maximum or descend to a minimum. There are many algorithms for computing the MLE, such as Newton-Raphson (`NR`), Quasi-Newton (`BFGS`), and Gradient Descent.

We can get coefficients by either:

1. **Maximizing** the log-likelihood, or
2. **Minimizing** the negative log-likelihood


# Gradient Descent

In this section, we **minimize** the negative log-likelihood to arrive at our parameters that most likely generated our data using gradient descent. Gradient descent is optimal because it avoids computing the hessian (the second derivative) which can be computationally expensive or difficult given certain functions. Instead, it uses the gradient to choose the direction for the next update. Gradient descent is very popular for machine learning methods, particularly for high dimensional computation *because* it avoids computing the hessian. 

To find our coefficients, we need to compute two quantities of interest: (1) the negative log-likelihood, and (2) its first derivative, the gradient. Gradient descent, often called "standard" or "batch" gradient descent uses all data points for computing the next search direction. In the next section, we show how we can circumvent this by using Stochastic Gradient Descent.

## Quantity of Interest: Negative Log-Likelihood

The log-likelihood is: $$\text{log} L(\theta|y) = \sum_{i=1}^n [y_{i}*log(\theta_{i}) + (1-y_{i})*log(1-\theta_{i})] $$ which, programmed into R, looks like the following:

```{r}
negLL <- function(b,X,y){  # b = betas
    p <- as.vector(1/(1+exp(-X %*% b)))  # "standard logistic function"; 1/1+exp(-X)
    -sum(y*log(p) + (1-y)*log(1-p))   # negative log-likelihood
    } 
```


## Quantity of Interest: Gradient

The gradient is: $$\sum_{i=1}^n (y_{i} - \theta_{i})*X_{ij} $$ which, programmed into R, looks like the following:


```{r}
gradient <- function(b,X,y){
  p <- as.vector(1/(1+exp(-X %*% b)))
  -apply(((y - p)*X),2,sum) # derivative of cost function: (p) = y-hat
  } # gradient of the negative log-likelihood; a vector of partial derivatives
```

## Gradient Descent Optimization: In Steps

Next, we step through the optimization algorithm manually so we can better understand how our coefficients get updated as we minimize our negative log-likelihood.

First, we instantiate some initial values for our coefficients and a learning rate.

```{r}
beta = c(0,0,0)  # for intercept and two betas
learning_rate = 0.0001  # learning rate
```

Next, we step through the process iteratively before proceeding to a while loop. In the code below, we:

1. Save the current beta coefficients separately as the old,

2. Multiply our learning rate by the first derivative, obtaining the values at which we nudge our coefficients, either positively or negatively,

3. Update our current beta by subtracting the error,

4. Use the new betas to update our negative log-likelihood, and

5. Calculate the euclidean distance between our beta estimates which we will use to stop iterating once we reach a specified tolerance.

```{r}
# save the previous value
beta0 = beta

# calculate h, the increment
h = learning_rate*gradient(beta, X, y)

# update beta
beta = beta - h # subtract to minimize

# update the log likelihood 
logL = negLL(beta, X, y)

# calculate the euclidean distance
eps  = sqrt(sum((beta - beta0)^2))
```


## Gradient Descent Optimization: Automated

In this section, we utilize a while loop to execute our numerical optimization using gradient descent. This loop will continue until the euclidean distance between our $\beta$s stop updating past our specified tolerance.


```{r}
tol = 10^-6  # set the threshold for convergence
beta = c(0,0,0)  # for intercept and two betas
maxit = 1000
iter = 0
learning_rate = 0.0001  # learning rate
eps = Inf

start = Sys.time()
while(eps > tol & iter < maxit){
  # save the previous value
  beta0 = beta
  
  # calculate h, the increment
  h = learning_rate*gradient(beta, X, y)
  
  # update beta
  beta = beta - h # subtract to minimize
  
  # update the log likelihood 
  logL = negLL(beta, X, y)
  
  # calculate the euclidean distance
  eps  = sqrt(sum((beta - beta0)^2))
  
  # update the iteration number
  iter = iter + 1
  if(iter == maxit) warning("Iteration limit reached without convergence")
  
  # print out info to keep track
  if(floor(iter/20) == ceiling(iter/20)) cat(sprintf("Iter: %d logL: %.2f beta0: %.3f beta1: %.3f beta2: %.3f eps:%f\n",iter, logL,beta[1],beta[2],beta[3],eps))
}
```

As we can see, we have: 

1. Minimized our negative log-likelihood, and 

2. Found our estimates of beta iteratively until we reached our specified tolerance (0.000001).

To compare our results with that of R's `glm` function, consider the following:

```{r}
fit.glm <- glm(y ~ x1 + x2, data = data.frame(X), family=binomial(link="logit")) 
round(fit.glm$coefficients, 3)
```

As we can see, we get identical results.


# Stochastic Gradient Descent

Stochastic gradient descent randomly sub-samples the data and performs an update for $\beta$ with a random observation drawn from the data, rather than a full pass over the data. The same procedure continues as above, except we sample our data and iterate until convergence. While in this example stochastic gradient descent will perform slower than standard gradient descent, in situations where we have more data and higher dimensional features, stochastic gradient descent will outperform gradient descent significantly by reducing computation time.

# Stochastic Gradient Descent Optimization: Automated

```{r}
# the start
tol = 10^-9  # lowered threshold to ensure convergence
beta = c(0,0,0)  # for intercept and two betas
maxit = 100000  # increased iteration limit
iter = 0
learning_rate = 0.0001
subsample.size = 10  # sample 10 observations a time
eps = 100000
epscount = 0
logL = rep(NA, maxit)

# given this, the result below will be the same as NR
start = Sys.time()
while(eps > tol & iter < maxit & epscount < 4){
  # save the previous value
  beta0 = beta
  
  # take subsample
  index = sample(1:500, size = subsample.size,replace = T)
  
  # calculate h, the increment
  h = learning_rate*gradient(beta, X[index,], y[index])
  
  # update lambda
  beta = beta - h
  
  # update the log likelihood 
  logL[iter] = negLL(beta, X, y)
  
  # use relative change in logL from 1000 iterations prior
  # this is because randomness between single iterations large, smooths out
  if(iter > 1000) eps  = abs(logL[iter] -logL[iter-1000])/abs(logL[iter-1000])
  
  # we use this count to protect against randomly hitting the convergene limit early
  if(eps < tol) epscount = epscount+1
  
  # update the iteration number
  iter = iter + 1
  if(iter == maxit) warning("Iteration limit reached without convergence")
  
  # print out info to keep track
  if(floor(iter/200) == ceiling(iter/200)) cat(sprintf("Iter: %d logL: %.2f beta0: %.3f beta1: %.3f beta2: %.3f eps:%f\n",iter, logL[iter-1],beta[1],beta[2],beta[3],eps))
}
```

To compare with the `GLM` function's output again:

```{r}
round(fit.glm$coefficients, 3)
```

As we can see, we arrive at similar results, albeit in a slower and more circuitous manner given the "stochastic" nature of our sampling process.




# Quasi Newton: BFGS

To show how we can maximize our log-likelihood and arrive at similar results by using R's `optim` package, we make some slight changes to the code below:

1. We remove the negative sign from likelihood function and its gradient, and

2. We specify `fnscale=-1` inside `optim` to tell the algorithm to **maximize** rather than **minimize**.


```{r}
LL <- function(b,X,y){  # b = betas
    p<-as.vector(1/(1+exp(-X %*% b)))  # "standard logistic function"; 1/1+exp(-X)
    sum(y*log(p) + (1-y)*log(1-p))   # log-likelihood
}

gradient <- function(b,X,y){
  p <- as.vector(1/(1+exp(-X %*% b)))
   apply(((y - p)*X),2,sum) # gradient of the log-likelihood function above
}
```

Next, we incorporate these functions into `optim` and output its results.

```{r}
results <- optim(rep(0, ncol(X)), fn=LL, gr=gradient,
                   hessian=T, method='BFGS', X=X, y=y,
                 control=list(trace=1,
                              REPORT=1,
                              fnscale=-1))

list(coefficients=results$par,var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)

round(results$par, 3)
```


To compare with the `GLM` function's output again:

```{r}
round(fit.glm$coefficients, 3)
```

As we can see, we find can find identical results either through maximization or minimization. Further, we can see that BFGS outperforms gradient descent here in terms of speed.






# Sources

* Ward, Michael D., and John S. Ahlquist. Maximum Likelihood for Social Science: Strategies for Analysis. Cambridge University Press, 2018.

* Giudici, Paolo, Geof H. Givens, and Bani K. Mallick. Wiley Series in Computational Statistics. Wiley Online Library, 2013.

* https://biodatascience.github.io/statcomp/optim/optim.html




